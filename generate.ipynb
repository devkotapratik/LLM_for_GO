{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.traversal.depth_first_search import dfs_tree\n",
    "\n",
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"/home/informatics/pdevkota\")\n",
    "DATA_DIR = Path.joinpath(BASE_DIR, \"data\")\n",
    "DATASET_DIR = Path.joinpath(DATA_DIR, \"model_input\", \"dataset\")\n",
    "FALCON_MODEL = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "MODEL_DIR = Path.joinpath(Path(\"./MODELS\"), Path(FALCON_MODEL.upper().replace(\"-\", \"_\")).stem) #\"MODELS/FALCON_7B_INSTRUCT\"\n",
    "model_name = str(MODEL_DIR)\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = PeftConfig.from_pretrained(model_name)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.base_model_name_or_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [i for i in DATASET_DIR.iterdir() if i.suffix==\".json\" and \"v4\" in str(i)]\n",
    "train_file = [i for i in data_files if \"train\" in str(i)][0]\n",
    "eval_file = [i for i in data_files if \"test\" in str(i)][0]\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": str(train_file),\n",
    "        \"eval\": str(eval_file)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotstrflist(example):\n",
    "    data = \"terms: none\\nGO concepts: none\\nparents: none\"\n",
    "    if len(example[\"annot\"]):\n",
    "        terms, concepts, parents = [], [], []\n",
    "        for j in example[\"annot\"]:\n",
    "            terms.append(j[\"spanned_text\"])\n",
    "            concepts.append(j[\"go_concept\"])\n",
    "            parent = [k[\"GO Concept\"] for k in j[\"parents\"]]\n",
    "            if len(parent) == 0:\n",
    "                parent= \"none\"\n",
    "            elif len(parent) == 1:\n",
    "                parent = parent[0]\n",
    "            else:\n",
    "                parent = f\"[{' | '.join(parent)}]\"\n",
    "            parents.append(parent)\n",
    "        assert len(terms) == len(concepts) == len(parents)\n",
    "        terms = \" | \".join(terms)\n",
    "        concepts = \" | \".join(concepts)\n",
    "        parents = \" | \".join(parents)\n",
    "        data = f\"terms: [{terms}]\\nGO concepts: [{concepts}]\\nparents: [{parents}]\"\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt_no_response(example):\n",
    "    pre_prompt = \"\"\"Gene Ontology (GO) is a widely used bioinformatics resource that provides a structured\n",
    "    vocabulary for annotating and categorizing genes and gene products based on their biological functions,\n",
    "    cellular locations, and molecular activities. You are a gene ontology expert and your objective is to use\n",
    "    your knowledge of the biological domain and the details provided below to write a response that appropriately\n",
    "    completes the instruction.\"\"\"\n",
    "    pre_prompt = re.sub(r\"\\s+\", \" \", pre_prompt)\n",
    "    \n",
    "    instruction = \"\"\"Use the input sentence below to label the tokens: terms, GO concepts and parents.\n",
    "    A term is a word or a phrase (phrase is a sequence of words) that represents a GO concept. Each term\n",
    "    MUST be present in the provided input sentence. A GO concept refers to a specific term or category with\n",
    "    GO hierarchy. Each GO concept can have zero or more parents. A parent represents immediate predecessor\n",
    "    of a GO concept. The response SHOULD have equal number of terms, GO concepts and parents.\"\"\"\n",
    "    instruction = re.sub(r\"\\s+\", \" \", instruction)\n",
    "    inp = example[\"pre\"]\n",
    "    prompt = f\"{pre_prompt}\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    output = annotstrflist(example)\n",
    "    response = prompt + output\n",
    "    encoded_full_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    encoded_full_prompt_and_response = tokenizer(response)\n",
    "    len_with_response = len(encoded_full_prompt_and_response[\"input_ids\"])\n",
    "    return {**encoded_full_prompt, \"output\": output, \"len_with_response\": len_with_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = dataset.map(generate_prompt_no_response, num_proc=os.cpu_count())\n",
    "new_dataset = new_dataset.filter(lambda x: x[\"len_with_response\"] < 400, num_proc=os.cpu_count())\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_config = GenerationConfig(\n",
    "    temperature=0.01,\n",
    "    top_k=0.85,\n",
    "    top_n=5,\n",
    "    num_beams=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = Path.joinpath(Path(\".\"), \"predictions\", MODEL_DIR.stem, f\"outputs_{int(device[-1])+1}.json\")\n",
    "Path.mkdir(output_path.parent, exist_ok=True, parents=True)\n",
    "\n",
    "part = int(device[-1]) + 1\n",
    "split_size = new_dataset[\"eval\"].num_rows // 3 + 1\n",
    "reqd_dataset = Dataset.from_dict(new_dataset[\"eval\"][split_size * (part - 1) : split_size * part])\n",
    "print(\"Dataset Loaded from {0}: {1}\".format(\n",
    "    split_size * (part - 1), split_size * part)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    if not Path(str(filename)).exists():\n",
    "        contents = []\n",
    "    else:\n",
    "        with open(filename, \"r\") as f:\n",
    "            contents = json.load(f)\n",
    "    contents.extend(data)\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimal_prompt(example):\n",
    "    pre_prompt = \"\"\"You are a gene ontology expert and your objective is to use\n",
    "    your knowledge of the biological domain and the details provided below to write a response that appropriately\n",
    "    completes the instruction.\"\"\"\n",
    "    pre_prompt = re.sub(r\"\\s+\", \" \", pre_prompt)\n",
    "    instruction = \"\"\"Use the input sentence below to label the tokens: terms, GO concepts and parents.\n",
    "    A term is a word or a phrase (phrase is a sequence of words) that represents a GO concept. Each term\n",
    "    MUST be present in the provided input sentence. A GO concept refers to a specific term or category with\n",
    "    GO hierarchy. Each GO concept can have zero or more parents. A parent represents immediate predecessor\n",
    "    of a GO concept. The response SHOULD have equal number of terms, GO concepts and parents.\"\"\"\n",
    "    instruction = re.sub(r\"\\s+\", \" \", instruction)\n",
    "    inp = example[\"pre\"]\n",
    "    prompt = f\"{pre_prompt}\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    encoded_full_prompt = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    return encoded_full_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(total=reqd_dataset.num_rows, desc=\"Generating responses from given prompts\")\n",
    "    checkpoint_idx = 0\n",
    "    if output_path.exists():\n",
    "        with open(output_path, \"r\") as f:\n",
    "            contents = json.load(f)\n",
    "        checkpoint_idx = len(contents)\n",
    "        pbar.update(checkpoint_idx)\n",
    "    chunk_data = []\n",
    "    for idx in range(checkpoint_idx, reqd_dataset.num_rows):\n",
    "        input_ids = minimal_prompt(reqd_dataset[idx])[\"input_ids\"]\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids.to(device),\n",
    "            generation_config=gen_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "            max_new_tokens=100,\n",
    "        )\n",
    "        response = tokenizer.decode(output.sequences[0]).strip().split(\"### Response:\\n\")[1].split(\"#\")[0].strip()\n",
    "        data = reqd_dataset[idx].copy()\n",
    "        [data.pop(i) for i in [\"input_ids\", \"attention_mask\", \"len_with_response\", \"token_type_ids\"]]\n",
    "        data.update({\"response\": response})\n",
    "        chunk_data.append(data)\n",
    "        if (idx + 1) % 1 == 0:\n",
    "            save_to_json(chunk_data, output_path)\n",
    "            chunk_data = []\n",
    "        pbar.update(1)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
